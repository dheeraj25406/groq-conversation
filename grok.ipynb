{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the Requirements"
      ],
      "metadata": {
        "id": "eMPeVqcMCrKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wH0L5nXiCX8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11899cd5-bd99-4276-87e9-862fee4f8819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Setup complete. Groq API client initialized.\n"
          ]
        }
      ],
      "source": [
        "# Install required package first, then import and setup\n",
        "\n",
        "# Install openai\n",
        "!pip install openai\n",
        "\n",
        "# Now import after installation\n",
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from typing import List, Dict, Any, Optional\n",
        "import math\n",
        "\n",
        "# Set Groq API key (provided by user)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Verify API key\n",
        "if not os.getenv(\"GROQ_API_KEY\"):\n",
        "    raise ValueError(\"GROQ_API_KEY is not set.\")\n",
        "\n",
        "# Initialize OpenAI client for Groq API\n",
        "client = OpenAI(\n",
        "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        ")\n",
        "print(\"Setup complete. Groq API client initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationManager Class"
      ],
      "metadata": {
        "id": "tlnqljddkKu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ConversationManager class with all necessary methods for Task 1\n",
        "\n",
        "from typing import List, Dict, Any, Optional  # Ensure typing imports are available\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, model: str = \"llama3-8b-8192\", summarize_every: int = 3):\n",
        "        self.history: List[Dict[str, str]] = []\n",
        "        self.model = model\n",
        "        self.summarize_every = summarize_every\n",
        "        self.turn_count = 0\n",
        "\n",
        "    def add_turn(self, role: str, content: str) -> None:\n",
        "        \"\"\"Add a user or assistant message to history.\"\"\"\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        self.turn_count += 1\n",
        "        if self.turn_count % self.summarize_every == 0:\n",
        "            self._summarize_history()\n",
        "\n",
        "    def truncate_by_turns(self, max_turns: int) -> None:\n",
        "        \"\"\"Truncate history to last max_turns messages.\"\"\"\n",
        "        if len(self.history) > max_turns:\n",
        "            self.history = self.history[-max_turns:]\n",
        "\n",
        "    def truncate_by_length(self, max_length: int, unit: str = \"chars\") -> None:\n",
        "        \"\"\"Truncate history to total max_length chars or words.\"\"\"\n",
        "        total = 0\n",
        "        if unit == \"chars\":\n",
        "            total = sum(len(msg[\"content\"]) for msg in self.history)\n",
        "        elif unit == \"words\":\n",
        "            total = sum(len(msg[\"content\"].split()) for msg in self.history)\n",
        "\n",
        "        if total > max_length:\n",
        "            while total > max_length and len(self.history) > 1:\n",
        "                removed = self.history.pop(0)\n",
        "                if unit == \"chars\":\n",
        "                    total -= len(removed[\"content\"])\n",
        "                elif unit == \"words\":\n",
        "                    total -= len(removed[\"content\"].split())\n",
        "\n",
        "    def _summarize_history(self) -> None:\n",
        "        \"\"\"Summarize history using Groq API and replace with summary.\"\"\"\n",
        "        if len(self.history) < 2:\n",
        "            return\n",
        "\n",
        "        history_text = \"\\n\".join([f\"{msg['role'].title()}: {msg['content']}\" for msg in self.history])\n",
        "        prompt = f\"Summarize the following conversation concisely, keeping key points and context:\\n\\n{history_text}\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "\n",
        "        self.history = [\n",
        "            {\"role\": \"system\", \"content\": f\"Summary of previous conversation: {summary}\"},\n",
        "            self.history[-1]\n",
        "        ]\n",
        "        print(f\"Summarization triggered! New history length: {len(self.history)}\")\n",
        "\n",
        "    def get_history(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"Return current history.\"\"\"\n",
        "        return self.history[:]\n",
        "\n",
        "    def chat(self, user_input: str) -> str:\n",
        "        \"\"\"Simulate a chat turn: add user input, generate response, handle summarization.\"\"\"\n",
        "        self.add_turn(\"user\", user_input)\n",
        "\n",
        "        messages = self.get_history()\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            max_tokens=200,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        assistant_reply = response.choices[0].message.content.strip()\n",
        "\n",
        "        self.add_turn(\"assistant\", assistant_reply)\n",
        "        return assistant_reply\n",
        "\n",
        "print(\"ConversationManager class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FigRchmbkLic",
        "outputId": "66862218-7659-4941-83f5-60114e3383eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConversationManager class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Conversation Demo"
      ],
      "metadata": {
        "id": "VEzKaVeAkO6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate conversation with no truncation, showing all turns\n",
        "# Updated model to 'llama-3.1-8b-instant' as 'llama3-8b-8192' is deprecated\n",
        "\n",
        "print(\"=== Task 1: Full Conversation Demo ===\")\n",
        "\n",
        "# Create manager with updated model (deprecated model replaced)\n",
        "manager = ConversationManager(model=\"llama-3.1-8b-instant\", summarize_every=3)\n",
        "\n",
        "# Sample conversation inputs\n",
        "sample_inputs = [\n",
        "    \"Hello, what's the weather like today?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"Explain quantum physics simply.\",\n",
        "    \"Recommend a book on AI.\",\n",
        "    \"What's your favorite color?\",\n",
        "]\n",
        "\n",
        "# Run conversation\n",
        "for i, inp in enumerate(sample_inputs, 1):\n",
        "    reply = manager.chat(inp)\n",
        "    print(f\"Turn {i}: User: {inp}\\nAssistant: {reply}\\n\")\n",
        "\n",
        "print(\"Final history:\")\n",
        "print(json.dumps(manager.get_history(), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiXQxcBukQr8",
        "outputId": "644d21d7-8fc2-4a1d-dda8-69b7363beb81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 1: Full Conversation Demo ===\n",
            "Turn 1: User: Hello, what's the weather like today?\n",
            "Assistant: However, I'm a large language model, I don't have real-time access to current weather conditions. I can suggest some options to help you find out the weather today:\n",
            "\n",
            "1. Check online weather websites: You can visit websites like AccuWeather, Weather.com, or the National Weather Service (NWS) for the current weather conditions in your area.\n",
            "2. Use a mobile app: Many mobile apps such as Dark Sky, Weather Underground, or The Weather Channel provide real-time weather updates.\n",
            "3. Check your phone's built-in weather app: If your phone has a built-in weather app, you can check the current weather conditions.\n",
            "4. Ask a voice assistant: You can ask a voice assistant like Siri, Google Assistant, or Alexa to provide you with the current weather conditions.\n",
            "\n",
            "Please let me know if there's anything else I can help you with.\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 2: User: Tell me a joke.\n",
            "Assistant: A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schrödinger's cat?\" The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 3: User: What's the capital of France?\n",
            "Assistant: The capital of France is Paris.\n",
            "\n",
            "Turn 4: User: Explain quantum physics simply.\n",
            "Assistant: Quantum physics is a branch of physics that studies the behavior of really small things like atoms and particles that are too tiny to see. At this scale, the normal rules of physics don't apply, and some strange and fascinating things happen.\n",
            "\n",
            "Here are a few key ideas to get you started:\n",
            "\n",
            "1. **Everything is made up of tiny particles**: These particles, called atoms and subatomic particles, can't be broken down any further. They're the building blocks of everything around us.\n",
            "\n",
            "2. **These particles can be in many places at once**: This is known as superposition. Think of it like a coin that's both heads and tails at the same time.\n",
            "\n",
            "3. **These particles can be connected even when they're really far apart**: This is known as entanglement. Imagine two dancers who are connected by an invisible string. If one dancer moves, the other dancer will move too, even if they're on opposite sides of the stage.\n",
            "\n",
            "4. **Measuring these particles\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 5: User: Recommend a book on AI.\n",
            "Assistant: Based on your interest in AI, I'd like to recommend the following books:\n",
            "\n",
            "1. **\"Life 3.0: Being Human in the Age of Artificial Intelligence\" by Max Tegmark**: This book explores the possibilities and implications of AI surpassing human intelligence. Tegmark, a physicist and AI researcher, provides a comprehensive overview of the field and its potential impact on society.\n",
            "\n",
            "2. **\"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World\" by Pedro Domingos**: This book delves into the concept of a \"master algorithm\" that could potentially solve all machine learning problems. Domingos, a computer scientist, explains how AI is changing the way we live and work.\n",
            "\n",
            "3. **\"Superintelligence: Paths, Dangers, Strategies\" by Nick Bostrom**: This book focuses on the risks and challenges associated with developing superintelligent AI. Bostrom, a philosopher and director of the Future of Humanity Institute, presents a\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 6: User: What's your favorite color?\n",
            "Assistant: I don't have a personal preference, including a favorite color. I'm designed to provide information and assist with tasks, but I don't have personal experiences, emotions, or opinions. I'm here to help answer your questions and provide information on a wide range of topics.\n",
            "\n",
            "Final history:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: The conversation started with a geography question about the capital of France, which was answered as \\\"Paris.\\\" The user then asked for an explanation of quantum physics, which was simplified by the assistant. Following this, the user asked for a book recommendation on AI, and the assistant provided three book suggestions:\\n\\n1. \\\"Life 3.0: Being Human in the Age of Artificial Intelligence\\\" by Max Tegmark\\n2. \\\"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World\\\" by Pedro Domingos\\n3. \\\"Superintelligence: Paths, Dangers, Strategies\\\" by Nick Bostrom\\n\\nFinally, the user asked the assistant about its favorite color, and the assistant replied that it doesn't have personal preferences, including a favorite color, as it's designed to provide information and assist with tasks without personal experiences or emotions.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"I don't have a personal preference, including a favorite color. I'm designed to provide information and assist with tasks, but I don't have personal experiences, emotions, or opinions. I'm here to help answer your questions and provide information on a wide range of topics.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 - Truncation by Turns Demo"
      ],
      "metadata": {
        "id": "jpHiwe2NkT50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate truncation by keeping last 4 turns\n",
        "# Note: sample_inputs must be defined from Cell 3 or re-defined here\n",
        "\n",
        "print(\"=== Task 1: Truncation by Turns (Last 4) ===\")\n",
        "\n",
        "# Re-define sample_inputs if not available from previous cell\n",
        "sample_inputs = [\n",
        "    \"Hello, what's the weather like today?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"Explain quantum physics simply.\",\n",
        "    \"Recommend a book on AI.\",\n",
        "    \"What's your favorite color?\",\n",
        "]\n",
        "\n",
        "# Create manager with updated model and disable periodic summarization for clarity\n",
        "manager_trunc_turns = ConversationManager(model=\"llama-3.1-8b-instant\", summarize_every=5)\n",
        "\n",
        "# Run full conversation\n",
        "print(\"Running full conversation...\")\n",
        "for inp in sample_inputs:\n",
        "    reply = manager_trunc_turns.chat(inp)\n",
        "    print(f\"User: {inp} -> Assistant: {reply[:50]}...\")  # Show truncated reply for brevity\n",
        "\n",
        "print(\"\\nBefore truncation - Full history length:\", len(manager_trunc_turns.get_history()))\n",
        "print(\"Before truncation - History:\")\n",
        "print(json.dumps(manager_trunc_turns.get_history(), indent=2)[:500] + \"...\" if len(json.dumps(manager_trunc_turns.get_history(), indent=2)) > 500 else json.dumps(manager_trunc_turns.get_history(), indent=2))\n",
        "\n",
        "# Apply truncation to keep only last 4 turns\n",
        "manager_trunc_turns.truncate_by_turns(4)\n",
        "\n",
        "print(\"\\nAfter truncation (last 4 turns):\")\n",
        "print(\"History length:\", len(manager_trunc_turns.get_history()))\n",
        "print(json.dumps(manager_trunc_turns.get_history(), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG9T12AzkWCV",
        "outputId": "3a682f31-17a0-4294-e195-8702cbf100e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 1: Truncation by Turns (Last 4) ===\n",
            "Running full conversation...\n",
            "User: Hello, what's the weather like today? -> Assistant: However, I'm a large language model, I don't have ...\n",
            "User: Tell me a joke. -> Assistant: Here's one:\n",
            "\n",
            "What do you call a fake noodle?\n",
            "\n",
            "An i...\n",
            "Summarization triggered! New history length: 2\n",
            "User: What's the capital of France? -> Assistant: The capital of France is Paris....\n",
            "User: Explain quantum physics simply. -> Assistant: Quantum physics is a branch of physics that studie...\n",
            "Summarization triggered! New history length: 2\n",
            "User: Recommend a book on AI. -> Assistant: Here are a few book recommendations on AI:\n",
            "\n",
            "1. **\"...\n",
            "User: What's your favorite color? -> Assistant: I don't have a favorite color. I'm a machine learn...\n",
            "\n",
            "Before truncation - Full history length: 4\n",
            "Before truncation - History:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: Here's a concise summary of the conversation:\\n\\n- The user initially asked for the capital of France, which the assistant replied was Paris.\\n- The user then asked for an explanation of quantum physics, which the assistant simplified by comparing classical and quantum physics, and highlighting key concepts such as the uncertainty principle, wave-particle duality, and superposition.\\n- Finally, the user asked for a boo...\n",
            "\n",
            "After truncation (last 4 turns):\n",
            "History length: 4\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: Here's a concise summary of the conversation:\\n\\n- The user initially asked for the capital of France, which the assistant replied was Paris.\\n- The user then asked for an explanation of quantum physics, which the assistant simplified by comparing classical and quantum physics, and highlighting key concepts such as the uncertainty principle, wave-particle duality, and superposition.\\n- Finally, the user asked for a book recommendation on AI, and the assistant suggested four books: \\\"Life 3.0\\\" by Max Tegmark, \\\"Deep Learning\\\" by Ian Goodfellow et al., \\\"Superintelligence\\\" by Nick Bostrom, and \\\"The Singularity Is Near\\\" by Ray Kurzweil, recommending \\\"Life 3.0\\\" as a starting point.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"Here are a few book recommendations on AI:\\n\\n1. **\\\"Life 3.0: Being Human in the Age of Artificial Intelligence\\\" by Max Tegmark**: This book explores the possibilities and challenges of AI, and how it will affect humanity.\\n2. **\\\"Deep Learning\\\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville**: This book provides a comprehensive introduction to the field of deep learning, which is a key area of AI research.\\n3. **\\\"Superintelligence: Paths, Dangers, Strategies\\\" by Nick Bostrom**: This book explores the risks and benefits of creating superintelligent machines, and how we can prepare for a future with potentially transformative AI.\\n4. **\\\"The Singularity Is Near: When Humans Transcend Biology\\\" by Ray Kurzweil**: This book explores the idea of a technological singularity, where AI surpasses human intelligence and brings about a revolution in technology and society.\\n\\nI recommend starting with \\\"Life\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's your favorite color?\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"I don't have a favorite color. I'm a machine learning model, and I don't have personal preferences or emotions. I'm designed to provide information and assist with tasks, but I don't have subjective experiences or opinions.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 - Truncation by Length Demo"
      ],
      "metadata": {
        "id": "1LfOVJLmkZ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate truncation by character limit (500 chars)\n",
        "# Note: sample_inputs must be defined from previous cells or re-defined here\n",
        "\n",
        "print(\"=== Task 1: Truncation by Length (500 chars) ===\")\n",
        "\n",
        "# Re-define sample_inputs if not available from previous cell\n",
        "sample_inputs = [\n",
        "    \"Hello, what's the weather like today?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"Explain quantum physics simply.\",\n",
        "    \"Recommend a book on AI.\",\n",
        "    \"What's your favorite color?\",\n",
        "]\n",
        "\n",
        "# Create manager with updated model and disable periodic summarization for clarity\n",
        "manager_trunc_len = ConversationManager(model=\"llama-3.1-8b-instant\", summarize_every=5)\n",
        "\n",
        "# Run full conversation\n",
        "print(\"Running full conversation...\")\n",
        "for inp in sample_inputs:\n",
        "    reply = manager_trunc_len.chat(inp)\n",
        "    print(f\"User: {inp} -> Assistant: {reply[:50]}...\")  # Show truncated reply for brevity\n",
        "\n",
        "# Calculate total characters before truncation\n",
        "full_history = manager_trunc_len.get_history()\n",
        "total_chars_before = sum(len(msg[\"content\"]) for msg in full_history)\n",
        "print(f\"\\nBefore truncation - Full history length: {len(full_history)} messages\")\n",
        "print(f\"Before truncation - Total characters: {total_chars_before}\")\n",
        "print(\"Before truncation - History preview:\")\n",
        "print(json.dumps(full_history, indent=2)[:500] + \"...\" if len(json.dumps(full_history, indent=2)) > 500 else json.dumps(full_history, indent=2))\n",
        "\n",
        "# Apply truncation by character limit\n",
        "manager_trunc_len.truncate_by_length(500, \"chars\")\n",
        "truncated_history = manager_trunc_len.get_history()\n",
        "total_chars_after = sum(len(msg[\"content\"]) for msg in truncated_history)\n",
        "\n",
        "print(f\"\\nAfter truncation (max 500 chars):\")\n",
        "print(f\"History length: {len(truncated_history)} messages\")\n",
        "print(f\"Total characters: {total_chars_after}\")\n",
        "print(\"Truncated history:\")\n",
        "print(json.dumps(truncated_history, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXeHpp7ykb6O",
        "outputId": "f3052707-0cc3-4df5-c4ec-eddf89f3522f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 1: Truncation by Length (500 chars) ===\n",
            "Running full conversation...\n",
            "User: Hello, what's the weather like today? -> Assistant: However, I'm a large language model, I don't have ...\n",
            "User: Tell me a joke. -> Assistant: Why don't scientists trust atoms?\n",
            "\n",
            "Because they ma...\n",
            "Summarization triggered! New history length: 2\n",
            "User: What's the capital of France? -> Assistant: The capital of France is Paris....\n",
            "User: Explain quantum physics simply. -> Assistant: Quantum physics is a branch of physics that studie...\n",
            "Summarization triggered! New history length: 2\n",
            "User: Recommend a book on AI. -> Assistant: Here are some popular and highly-recommended books...\n",
            "User: What's your favorite color? -> Assistant: I don't have a personal preference, including favo...\n",
            "\n",
            "Before truncation - Full history length: 4 messages\n",
            "Before truncation - Total characters: 1816\n",
            "Before truncation - History preview:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: Here's a concise summary of the conversation:\\n\\nThe conversation started with the user asking for the capital of France, which the assistant answered as Paris. The user then asked the assistant to explain quantum physics in simple terms, covering key concepts such as wave-particle duality, uncertainty principle, superposition, and entanglement.\\n\\nAfter that, the user asked for a book recommendation on AI, and the ass...\n",
            "\n",
            "After truncation (max 500 chars):\n",
            "History length: 2 messages\n",
            "Total characters: 295\n",
            "Truncated history:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"What's your favorite color?\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"I don't have a personal preference, including favorite colors. I'm designed to provide information and assist with tasks, but I don't have personal experiences, emotions, or opinions. I can provide information about colors, color theory, and more if you're interested.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 - Periodic Summarization Demo"
      ],
      "metadata": {
        "id": "6UKPfOf3kd68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate summarization every 3 turns\n",
        "# Note: sample_inputs must be defined from previous cells or re-defined here\n",
        "\n",
        "print(\"=== Task 1: Periodic Summarization (Every 3 Turns) ===\")\n",
        "\n",
        "# Re-define sample_inputs if not available from previous cell\n",
        "sample_inputs = [\n",
        "    \"Hello, what's the weather like today?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"Explain quantum physics simply.\",\n",
        "    \"Recommend a book on AI.\",\n",
        "    \"What's your favorite color?\",\n",
        "]\n",
        "\n",
        "# Create manager with updated model and summarization every 3 turns\n",
        "manager_periodic = ConversationManager(model=\"llama-3.1-8b-instant\", summarize_every=3)\n",
        "\n",
        "# Run conversation for first 6 inputs (to trigger 2 summaries)\n",
        "for i, inp in enumerate(sample_inputs[:6], 1):\n",
        "    reply = manager_periodic.chat(inp)\n",
        "    print(f\"Turn {i}: User: {inp}\\nAssistant: {reply}\\n\")\n",
        "    if i % 3 == 0:\n",
        "        print(f\"--- Summary after turn {i} ---\")\n",
        "        print(\"Current history after summary:\")\n",
        "        print(json.dumps(manager_periodic.get_history(), indent=2))\n",
        "\n",
        "print(\"Final history after all summaries:\")\n",
        "print(json.dumps(manager_periodic.get_history(), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RqHRI-jkfok",
        "outputId": "cd504d98-beed-4cbc-fdf0-d735bf88a5e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 1: Periodic Summarization (Every 3 Turns) ===\n",
            "Turn 1: User: Hello, what's the weather like today?\n",
            "Assistant: However, I'm a large language model, I don't have real-time access to your current location's weather. But I can suggest a few options to help you find out the weather today:\n",
            "\n",
            "1. **Check online weather websites**: You can visit websites like AccuWeather, Weather.com, or the National Weather Service (NWS) to get the current weather conditions and forecast for your area.\n",
            "2. **Use a mobile app**: You can download a weather app on your smartphone, such as Dark Sky or Weather Underground, which can provide you with real-time weather updates and forecasts.\n",
            "3. **Ask a voice assistant**: If you have a smart speaker or virtual assistant, such as Siri, Google Assistant, or Alexa, you can ask it to tell you the weather in your area.\n",
            "\n",
            "If you provide me with your location or city, I can give you a general idea of the weather based on historical data.\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 2: User: Tell me a joke.\n",
            "Assistant: A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schrödinger's cat?\" The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 3: User: What's the capital of France?\n",
            "Assistant: The capital of France is Paris.\n",
            "\n",
            "--- Summary after turn 3 ---\n",
            "Current history after summary:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: The conversation started with the user asking about the current weather, but the assistant couldn't provide real-time information. It suggested alternatives to find the weather. The conversation then shifted to the user asking the assistant to tell a joke, which it did. After the joke, the user asked a geography question, \\\"What's the capital of France?\\\", and the assistant replied with the correct answer, \\\"Paris\\\".\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"The capital of France is Paris.\"\n",
            "  }\n",
            "]\n",
            "Turn 4: User: Explain quantum physics simply.\n",
            "Assistant: Quantum physics is a complex topic, but I'll try to break it down in simple terms.\n",
            "\n",
            "**What is quantum physics?**\n",
            "\n",
            "Quantum physics is a branch of physics that studies the behavior of tiny particles, such as atoms and subatomic particles (like electrons and photons). These particles are too small to see, and they behave differently than the objects we interact with in everyday life.\n",
            "\n",
            "**Key concepts:**\n",
            "\n",
            "1. **Wave-particle duality**: Quantum particles can exhibit both wave-like and particle-like behavior. Imagine a particle that can be a wave and a particle at the same time.\n",
            "2. **Uncertainty principle**: It's impossible to know certain properties of a quantum particle, like its position and speed, simultaneously with infinite precision.\n",
            "3. **Superposition**: Quantum particles can exist in multiple states at the same time. Think of it like spinning a coin: it can be heads or tails, but not both... or can it?\n",
            "4. **Entanglement**: When two\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 5: User: Recommend a book on AI.\n",
            "Assistant: I'm glad we were able to save that question for now. Here are some popular and highly-recommended books on AI:\n",
            "\n",
            "1. **\"Life 3.0: Being Human in the Age of Artificial Intelligence\"** by Max Tegmark: This book explores the possibilities and risks of AI, and how it will impact our lives in the future.\n",
            "2. **\"Deep Learning\"** by Ian Goodfellow, Yoshua Bengio, and Aaron Courville: This comprehensive textbook covers the fundamentals of deep learning, a key area of AI research.\n",
            "3. **\"Superintelligence: Paths, Dangers, Strategies\"** by Nick Bostrom: This book examines the potential risks and benefits of advanced AI, and discusses strategies for mitigating its negative consequences.\n",
            "4. **\"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World\"** by Pedro Domingos: This book explores the idea of a single, universal algorithm that could solve a wide\n",
            "\n",
            "Summarization triggered! New history length: 2\n",
            "Turn 6: User: What's your favorite color?\n",
            "Assistant: I don't have a personal preference, including a favorite color. I'm designed to provide information and assist with tasks, but I don't have subjective experiences or emotions like humans do. I can, however, tell you about different colors, color theories, and even provide information on color psychology if you're interested.\n",
            "\n",
            "--- Summary after turn 6 ---\n",
            "Current history after summary:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: Here's a concise summary of the conversation:\\n\\n- The conversation started with an unfulfilled request for real-time weather information.\\n- The assistant answered a geography question about the capital of France and told a joke.\\n- It explained key concepts in quantum physics, including wave-particle duality, uncertainty principle, superposition, and entanglement.\\n- The user asked for a book recommendation on AI, and the assistant provided four popular and highly-recommended books:\\n  1. \\\"Life 3.0: Being Human in the Age of Artificial Intelligence\\\" by Max Tegmark\\n  2. \\\"Deep Learning\\\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\n  3. \\\"Superintelligence: Paths, Dangers, Strategies\\\" by Nick Bostrom\\n  4. \\\"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World\\\" by Pedro Domingos\\n- The user then asked the assistant's favorite color, to which the assistant replied that it doesn't have personal preferences, but can provide information on colors and color theories if needed.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"I don't have a personal preference, including a favorite color. I'm designed to provide information and assist with tasks, but I don't have subjective experiences or emotions like humans do. I can, however, tell you about different colors, color theories, and even provide information on color psychology if you're interested.\"\n",
            "  }\n",
            "]\n",
            "Final history after all summaries:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of previous conversation: Here's a concise summary of the conversation:\\n\\n- The conversation started with an unfulfilled request for real-time weather information.\\n- The assistant answered a geography question about the capital of France and told a joke.\\n- It explained key concepts in quantum physics, including wave-particle duality, uncertainty principle, superposition, and entanglement.\\n- The user asked for a book recommendation on AI, and the assistant provided four popular and highly-recommended books:\\n  1. \\\"Life 3.0: Being Human in the Age of Artificial Intelligence\\\" by Max Tegmark\\n  2. \\\"Deep Learning\\\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\n  3. \\\"Superintelligence: Paths, Dangers, Strategies\\\" by Nick Bostrom\\n  4. \\\"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World\\\" by Pedro Domingos\\n- The user then asked the assistant's favorite color, to which the assistant replied that it doesn't have personal preferences, but can provide information on colors and color theories if needed.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"I don't have a personal preference, including a favorite color. I'm designed to provide information and assist with tasks, but I don't have subjective experiences or emotions like humans do. I can, however, tell you about different colors, color theories, and even provide information on color psychology if you're interested.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 2 - JSON Schema and Extraction Functions"
      ],
      "metadata": {
        "id": "umteBTmnkihc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define JSON schema and functions for structured extraction\n",
        "# Updated model to supported 'llama-3.1-8b-instant' to avoid deprecation errors\n",
        "\n",
        "# JSON Schema for personal info extraction\n",
        "extraction_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\", \"description\": \"User's full name\"},\n",
        "        \"email\": {\"type\": \"string\", \"description\": \"User's email address\"},\n",
        "        \"phone\": {\"type\": \"string\", \"description\": \"User's phone number\"},\n",
        "        \"location\": {\"type\": \"string\", \"description\": \"User's location (city/country)\"},\n",
        "        \"age\": {\"type\": \"integer\", \"description\": \"User's age\"}\n",
        "    },\n",
        "    \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"]\n",
        "}\n",
        "\n",
        "# Tool for function calling\n",
        "extraction_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"extract_personal_info\",\n",
        "        \"description\": \"Extract personal information from the chat.\",\n",
        "        \"parameters\": extraction_schema\n",
        "    }\n",
        "}\n",
        "\n",
        "def extract_info_from_chat(chat_text: str, model: str = \"llama-3.1-8b-instant\") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Extract structured info using Groq API function calling.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Extract personal details from the conversation using the provided tool.\"},\n",
        "        {\"role\": \"user\", \"content\": chat_text}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        tools=[extraction_tool],\n",
        "        tool_choice=\"auto\",\n",
        "        temperature=0.1,\n",
        "    )\n",
        "\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        tool_call = response.choices[0].message.tool_calls[0]\n",
        "        if tool_call.function.name == \"extract_personal_info\":\n",
        "            return json.loads(tool_call.function.arguments)\n",
        "    return None\n",
        "\n",
        "def validate_extraction(extracted: Dict[str, Any]) -> bool:\n",
        "    \"\"\"Validate extracted data against schema.\"\"\"\n",
        "    required = [\"name\", \"email\", \"phone\", \"location\", \"age\"]\n",
        "    if not all(key in extracted for key in required):\n",
        "        return False\n",
        "    if not isinstance(extracted[\"name\"], str) or not isinstance(extracted[\"email\"], str) or not isinstance(extracted[\"phone\"], str):\n",
        "        return False\n",
        "    if not isinstance(extracted[\"location\"], str) or not isinstance(extracted[\"age\"], int):\n",
        "        return False\n",
        "    if \"@\" not in extracted[\"email\"] or not extracted[\"phone\"].startswith((\"+\", \"1\")):\n",
        "        return False\n",
        "    if extracted[\"age\"] < 0 or extracted[\"age\"] > 150:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "print(\"Task 2 functions and schema defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBz7p1qdkkpV",
        "outputId": "e6f501c0-7178-4614-b9d2-342ba690184f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2 functions and schema defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 - Extraction and Validation Demo"
      ],
      "metadata": {
        "id": "nNFWPVRfkoeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate extraction and validation on 3 sample chats\n",
        "\n",
        "print(\"=== Task 2: Extraction and Validation ===\")\n",
        "sample_chats = [\n",
        "    \"Hi, I'm John Doe, 28 years old, living in New York, USA. My email is john.doe@example.com and phone is +1-555-1234.\",\n",
        "    \"Hello! My name is Alice Smith, age 35, from London, UK. Contact me at alice.smith@ukmail.com or +44-20-1234-5678.\",\n",
        "    \"Hey, Sarah Johnson here, 42, in Tokyo, Japan. Email: sarah.j@asia.net, phone: +81-3-1234-5678.\",\n",
        "]\n",
        "\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    print(f\"\\n--- Sample Chat {i} ---\")\n",
        "    print(f\"Chat: {chat}\")\n",
        "\n",
        "    extracted = extract_info_from_chat(chat)\n",
        "    if extracted:\n",
        "        print(\"Extracted:\", json.dumps(extracted, indent=2))\n",
        "        is_valid = validate_extraction(extracted)\n",
        "        print(f\"Validation: {'PASS' if is_valid else 'FAIL'}\")\n",
        "    else:\n",
        "        print(\"Extraction failed: No tool call.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDfgt2g-kqD1",
        "outputId": "52744501-8367-43ee-d8ef-38e01c0afc9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 2: Extraction and Validation ===\n",
            "\n",
            "--- Sample Chat 1 ---\n",
            "Chat: Hi, I'm John Doe, 28 years old, living in New York, USA. My email is john.doe@example.com and phone is +1-555-1234.\n",
            "Extracted: {\n",
            "  \"age\": 28,\n",
            "  \"email\": \"john.doe@example.com\",\n",
            "  \"location\": \"New York, USA\",\n",
            "  \"name\": \"John Doe\",\n",
            "  \"phone\": \"+1-555-1234\"\n",
            "}\n",
            "Validation: PASS\n",
            "\n",
            "--- Sample Chat 2 ---\n",
            "Chat: Hello! My name is Alice Smith, age 35, from London, UK. Contact me at alice.smith@ukmail.com or +44-20-1234-5678.\n",
            "Extracted: {\n",
            "  \"age\": 35,\n",
            "  \"email\": \"alice.smith@ukmail.com\",\n",
            "  \"location\": \"London, UK\",\n",
            "  \"name\": \"Alice Smith\",\n",
            "  \"phone\": \"+44-20-1234-5678\"\n",
            "}\n",
            "Validation: PASS\n",
            "\n",
            "--- Sample Chat 3 ---\n",
            "Chat: Hey, Sarah Johnson here, 42, in Tokyo, Japan. Email: sarah.j@asia.net, phone: +81-3-1234-5678.\n",
            "Extracted: {\n",
            "  \"age\": 42,\n",
            "  \"email\": \"sarah.j@asia.net\",\n",
            "  \"location\": \"Tokyo, Japan\",\n",
            "  \"name\": \"Sarah Johnson\",\n",
            "  \"phone\": \"+81-3-1234-5678\"\n",
            "}\n",
            "Validation: PASS\n"
          ]
        }
      ]
    }
  ]
}